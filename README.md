# Data Science References

A list of references I've found useful in my path to becoming a better and more well-rounded data scientist. 

## Timeseries Forecasting / Non-Linear Dynamical Systems Analysis

### Phase Space
- [Time-Delay Embedding Theorem](https://en.wikipedia.org/wiki/Takens%27s_theorem)
- [Phase Space Tutorial](http://www.agnld.uni-potsdam.de/~marwan/matlab-tutorials/html/phasespace.html)

### Dynamic Time Warping
- [Timeseries Similarity](http://www.maths.manchester.ac.uk/~mbbx2se2/Docs/Dynamic_time_warping(Steven_Elsworth).pdf)
- [Tutorial](https://sflscientific.com/data-science-blog/2016/6/3/dynamic-time-warping-time-series-analysis-ii)

### Filtering / Smoothing
- [Bayesian Filters Python Digital Book](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)
- [Particle Files + Unscented Kalman](https://perso.uclouvain.be/michel.verleysen/papers/ffm07sd2.pdf)

## Feature Learning / Metric Learning

### Siamese Triplet Networks
- [Powerpoint Deck](https://filebox.ece.vt.edu/~jbhuang/teaching/ece6554/sp17/lectures/Lecture_08_Siamese_Triplet_Networks.pdf)

### Skipgram
- [Kernel PCA + Skipgram](https://medium.com/@vishwanigupta/kpca-skip-gram-model-improving-word-embedding-a6a0cb7aad49)
- [PPMI for Similarity](https://www.kaggle.com/gabrielaltay/word-vectors-from-pmi-matrix)
- [Better Understanding Predict & Count Models](https://arxiv.org/pdf/1511.02024.pdf)

## Dimensionality Reduction

### Random Projections
- [Random Projections for Kernel Machines](https://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf)
- [Good Overview of Random Projections](https://www.cs.waikato.ac.nz/~bobd/ECML_Tutorial/ECML_handouts.pdf)
- [Nonlinear Random Projections (with Kernels)](https://www.cs.toronto.edu/~duvenaud/talks/random-kitchen-sinks-tutorial.pdf)

### Latent Factor Models, Embeddings, Collaborative Filtering

#### Latent Semantic Analysis
- [Comparison of PLSI & NMF](https://pdfs.semanticscholar.org/0062/b9ff8522498b34f467e36af218d87fcf5d9a.pdf)
- [Network Embeddings](https://arxiv.org/pdf/1808.02590.pdf)

#### Generative Models
- [Intuitively Understanding VAE's](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)
- [Using MMD w/ Kernel Mean Embeddings for VAEs](https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/)
- [Information Autoencoder Family (proof that primal problem of VAEs and GANs is mutual information)](http://auai.org/uai2018/proceedings/papers/361.pdf)
- [Autoencoding Beyond Pixels. Using a Learned Similarity Metric](https://arxiv.org/pdf/1512.09300.pdf)

#### Semi-supervised Data Labeling
- [Cluster-then-Label](https://www.nature.com/articles/s41598-018-24876-0)

## Reproducing Kernel Hilbert Spaces

### General Kernels
- [Gram Matrix](http://math.uga.edu/~rothstei/6120Spring2008/GramMatrix20080325.pdf)
- [MMD - Kernel Hypothesis Testing](http://shogun-toolbox.org/notebook/latest/mmd_two_sample_testing.html)

### Kernel Clustering
- [Kernel Clustering](http://www.cse.msu.edu/~cse902/S14/ppt/kernelClustering.pdf)

## Association Rule Mining

- [Deciphering Association Rule Mining](https://medium.com/data-science-group-iitr/association-rule-mining-deciphered-d818f1215b06)

